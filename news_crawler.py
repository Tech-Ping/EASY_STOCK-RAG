import requests
import time
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import os
import hashlib
from pinecone import Pinecone, ServerlessSpec
from langchain_openai import ChatOpenAI, OpenAIEmbeddings  
from langchain_core.documents import Document  
from langchain.chains.summarize import load_summarize_chain
from urllib.parse import parse_qs, urlparse

load_dotenv()

PINECONE_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENV = os.getenv("PINECONE_ENV")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
INDEX_NAME = "easystock"

pc = Pinecone(api_key=PINECONE_KEY)
existing_indexes = pc.list_indexes().names()
if INDEX_NAME not in existing_indexes:
    pc.create_index(
        name=INDEX_NAME,
        dimension=1536,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )
    time.sleep(5)
    print(f"âœ… '{INDEX_NAME}' ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")

TICKERS = {
    "ì‚¼ì„±ì „ì": "005930",
    "LGì—ë„ˆì§€ì†”ë£¨ì…˜": "373220",
    "SKí•˜ì´ë‹‰ìŠ¤": "000660",
    "í˜„ëŒ€ì°¨": "005380",
    "ë¹„ì—ì´ì¹˜ì•„ì´": "083650"
}

BASE_URL = "https://finance.naver.com/item/news.naver?code={}"
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
    ),
    "Referer": "https://finance.naver.com/",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Connection": "keep-alive"
}

# ì£¼ì‹ ìƒì„¸ > ì¢…ëª©ë³„ ë‰´ìŠ¤ ì¡°íšŒí•˜ëŠ” APIì—ì„œ ì“°ì´ëŠ” ì„œë¹„ìŠ¤ë‹¨ ì½”ë“œ
def fetch_latest_news(ticker_code):
    iframe_url = get_news_iframe_url(ticker_code)
    if not iframe_url:
        return []

    response = requests.get(iframe_url, headers=HEADERS)
    response.encoding = "euc-kr"
    if response.status_code != 200:
        print(f"iframe ìš”ì²­ ì‹¤íŒ¨! ìƒíƒœ ì½”ë“œ: {response.status_code}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    news_table = soup.find("table", class_="type5")
    if not news_table:
        print("ë‰´ìŠ¤ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return []

    news_rows = news_table.find_all("tr")
    news_items = []
    for row in news_rows:
        title_tag = row.find("a", class_="tit")
        date_tag = row.find("td", class_="date")
        if title_tag and date_tag:
            title = title_tag.text.strip()
            link = title_tag["href"]

            if "/item/news_read.naver?" in link:
                parsed_url = urlparse(link)
                query_params = parse_qs(parsed_url.query)
                office_id = query_params.get("office_id", [""])[0]
                article_id = query_params.get("article_id", [""])[0]
                if office_id and article_id:
                    link = f"https://n.news.naver.com/mnews/article/{office_id}/{article_id}"
                else:
                    link = f"https://finance.naver.com{link}"
            elif not link.startswith("http"):
                link = f"https://finance.naver.com{link}"

            date = date_tag.text.strip()
            news_items.append({
                "title": title,
                "link": link,
                "date": date
            })

        if len(news_items) >= 5:
            break

    return news_items


def summarize_news(article_text):
    try:
        llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0)

        prompt = (
            "ğŸ“° ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n\n"
            "âš ï¸ **ì¤‘ìš”**: ì˜ì–´ê°€ ì•„ë‹ˆë¼, ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "ğŸ’¡ ì¶”ê°€ ì„¤ëª… ì—†ì´, ìˆœìˆ˜í•œ ìš”ì•½ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n"
            f"{article_text}"
        )
        response = llm.invoke(prompt)
        summary = response.content.strip()

        return summary  
    
    except Exception as e:
        print(f"ğŸš¨ ìš”ì•½ ì‹¤íŒ¨: {e}")
        return "ìš”ì•½ ì‹¤íŒ¨"


def get_news_article(url):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    print(f"ğŸ”— Fetching article from: {url}")

    try:
        response = requests.get(url, headers=HEADERS)
        response.encoding = "utf-8"

        if response.status_code != 200:
            print(f"ğŸš¨ ê¸°ì‚¬ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨! ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return "ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        article_soup = BeautifulSoup(response.text, "html.parser")

        article_body = article_soup.select_one("#dic_area")

        if article_body:
            return article_body.text.strip()
        else:
            print("ğŸš¨ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return "ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except Exception as e:
        print(f"ğŸ”¥ ERROR fetching article: {e}")
        return "ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


def get_news_iframe_url(ticker_code):
    url = BASE_URL.format(ticker_code)
    response = requests.get(url, headers=HEADERS)
    response.encoding = "euc-kr"

    if response.status_code != 200:
        print(f"ğŸš¨ {ticker_code} í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨! ìƒíƒœ ì½”ë“œ: {response.status_code}")
        return None
    
    soup = BeautifulSoup(response.text, "html.parser")
    iframe_tag = soup.find("iframe", {"id": "news_frame"})
    if not iframe_tag:
        print(f"ğŸ«¢ {ticker_code} iframeì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None
    
    return "https://finance.naver.com" + iframe_tag["src"]


def get_latest_stock_news(ticker_name, ticker_code):
    iframe_url = get_news_iframe_url(ticker_code)
    if not iframe_url:
        return None

    response = requests.get(iframe_url, headers=HEADERS)
    response.encoding = "euc-kr"

    if response.status_code != 200:
        print(f"ğŸš¨ iframe ìš”ì²­ ì‹¤íŒ¨! ìƒíƒœ ì½”ë“œ: {response.status_code}")
        return None

    soup = BeautifulSoup(response.text, "html.parser")
    news_table = soup.find("table", class_="type5")

    if not news_table:
        print(f"ğŸ«¢ ë‰´ìŠ¤ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ ({ticker_code})")
        return None

    first_news = news_table.find("tr", class_="first")
    if not first_news:
        print("ğŸš¨ ì²« ë²ˆì§¸ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None

    title_tag = first_news.find("a", class_="tit")
    if not title_tag:
        print("ğŸš¨ ê¸°ì‚¬ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None

    title = title_tag.text.strip()
    link = title_tag["href"]

    if "/item/news_read.naver?" in link:
        parsed_url = urlparse(link)
        query_params = parse_qs(parsed_url.query)

        office_id = query_params.get("office_id", [""])[0]
        article_id = query_params.get("article_id", [""])[0]

        if office_id and article_id:
            link = f"https://n.news.naver.com/mnews/article/{office_id}/{article_id}"
        else:
            link = f"https://finance.naver.com{link}"  

    elif not link.startswith("http"):
        link = f"https://finance.naver.com{link}"

    date_tag = first_news.find("td", class_="date")
    news_date = date_tag.text.strip() if date_tag else "Unknown"

    article_text = get_news_article(link)
    summary = summarize_news(article_text)

    return {
        "title": title,
        "link": link,  
        "summary": summary,
        "date": news_date,
        "ticker": ticker_name
    }


def generate_ascii_id(ticker, title, date):
    """ğŸ”— ì¢…ëª©ëª… + ë‰´ìŠ¤ ì œëª© + ë‚ ì§œ ì¡°í•©ìœ¼ë¡œ ë²¡í„° ID ìƒì„± (ì¤‘ë³µ ë°©ì§€)"""
    raw_text = f"{ticker}_{title}_{date}"
    return hashlib.sha256(raw_text.encode("utf-8")).hexdigest()[:16]

def store_latest_news(tickers_to_fetch):
    stored_news = []
    for name, code in tickers_to_fetch.items():
        print(f"ğŸ“¡ Fetching latest news for {name} ({code})...")
        latest_news = get_latest_stock_news(name, code)
        if latest_news:
            embeddings = OpenAIEmbeddings()
            index = pc.Index(INDEX_NAME)

            vector_id = generate_ascii_id(latest_news["ticker"], latest_news["title"], latest_news["date"])

            vector = embeddings.embed_query(str(latest_news["summary"])) 

            index.upsert([
                {
                    "id": vector_id,  
                    "values": vector,
                    "metadata": {
                        "title": latest_news["title"],
                        "url": latest_news["link"],
                        "date": latest_news["date"],
                        "summary": latest_news["summary"], 
                        "ticker": latest_news["ticker"]  
                    }
                }
            ])

            print(f"âœ… Latest news stored in Pinecone: {latest_news['title']} (ID: {vector_id})")
            stored_news.append(latest_news)
    return stored_news